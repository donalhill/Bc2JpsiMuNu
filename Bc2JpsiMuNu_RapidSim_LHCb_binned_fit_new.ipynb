{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donalrinho/Bc2JpsiMuNu/blob/main/Bc2JpsiMuNu_RapidSim_LHCb_binned_fit_new.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Binned fit of the reconstructed decay angles using template histograms\n",
        "\n",
        "In this notebook, we fit the reconstructed decay angles in $B_c^+ \\to J/\\psi \\mu^+ \\nu_\\mu$ MC using a binned method. In this fit, we model each angular term as a separate histogram template, using the histograms we made in the previous notebook. Each of these templates get multiplied by their corresponding angular observable values (the helicity amplitudes) in order to build the total fit. Just like we did in our unbinned fit to the true decay angles in RapidSim, we want to measure the helicity amplitdue magnitudes and phases. "
      ],
      "metadata": {
        "id": "6DBM0RXyMdcp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "bFIaN_bQTnRv"
      },
      "outputs": [],
      "source": [
        "!pip install -q uproot\n",
        "!pip install -q tensorflow==2.6.2 #specific versions for compatability with zfit\n",
        "!pip install -q hist\n",
        "!pip install -q mplhep\n",
        "!pip install -q zfit --pre #--pre gives us the alpha release of zfit which has the binned fit tools\n",
        "!pip install -q uncertainties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "NH6Qj7zOT7S_"
      },
      "outputs": [],
      "source": [
        "import uproot\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import zfit\n",
        "import hist\n",
        "from hist import Hist\n",
        "import mplhep\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import random\n",
        "from typing import Optional\n",
        "from zfit.core.space import supports\n",
        "import zfit.z.numpy as znp\n",
        "from uncertainties import *"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load the histograms we made for each angular term in the previous notebook. These will serve as our template PDFs, which we will add together to form the total fit PDF."
      ],
      "metadata": {
        "id": "RmYxfjOWMjJt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLxdqDy5TedV",
        "outputId": "192a330f-5437-4201-ab3b-518c6d47a1a5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: Hist(\n",
              "   Variable([-1, -0.79216, -0.58822, -0.38891, -0.19395, 0.00055617, 0.19486, 0.39035, 0.5888, 0.79242, 1], name='costheta_Jpsi_reco', label='costheta_Jpsi_reco'),\n",
              "   Variable([-1, -0.46394, -0.12545, 0.13095, 0.33459, 0.50304, 0.64358, 0.76183, 0.861, 0.94157, 1], name='costheta_W_reco', label='costheta_W_reco'),\n",
              "   Variable([-3.14159, -2.42962, -1.84089, -1.30021, -0.71122, 0.0043844, 0.71329, 1.29936, 1.83985, 2.42724, 3.14159], name='chi_reco', label='chi_reco'),\n",
              "   storage=Weight()) # Sum: WeightedSum(value=1, variance=1.42964e-06) (WeightedSum(value=1.0012, variance=1.43138e-06) with flow),\n",
              " 1: Hist(\n",
              "   Variable([-1, -0.79216, -0.58822, -0.38891, -0.19395, 0.00055617, 0.19486, 0.39035, 0.5888, 0.79242, 1], name='costheta_Jpsi_reco', label='costheta_Jpsi_reco'),\n",
              "   Variable([-1, -0.46394, -0.12545, 0.13095, 0.33459, 0.50304, 0.64358, 0.76183, 0.861, 0.94157, 1], name='costheta_W_reco', label='costheta_W_reco'),\n",
              "   Variable([-3.14159, -2.42962, -1.84089, -1.30021, -0.71122, 0.0043844, 0.71329, 1.29936, 1.83985, 2.42724, 3.14159], name='chi_reco', label='chi_reco'),\n",
              "   storage=Weight()) # Sum: WeightedSum(value=4.14867, variance=6.29795e-05) (WeightedSum(value=4.15916, variance=6.3183e-05) with flow),\n",
              " 2: Hist(\n",
              "   Variable([-1, -0.79216, -0.58822, -0.38891, -0.19395, 0.00055617, 0.19486, 0.39035, 0.5888, 0.79242, 1], name='costheta_Jpsi_reco', label='costheta_Jpsi_reco'),\n",
              "   Variable([-1, -0.46394, -0.12545, 0.13095, 0.33459, 0.50304, 0.64358, 0.76183, 0.861, 0.94157, 1], name='costheta_W_reco', label='costheta_W_reco'),\n",
              "   Variable([-3.14159, -2.42962, -1.84089, -1.30021, -0.71122, 0.0043844, 0.71329, 1.29936, 1.83985, 2.42724, 3.14159], name='chi_reco', label='chi_reco'),\n",
              "   storage=Weight()) # Sum: WeightedSum(value=3.98394, variance=2.73988e-05) (WeightedSum(value=3.98606, variance=2.74106e-05) with flow),\n",
              " 3: Hist(\n",
              "   Variable([-1, -0.79216, -0.58822, -0.38891, -0.19395, 0.00055617, 0.19486, 0.39035, 0.5888, 0.79242, 1], name='costheta_Jpsi_reco', label='costheta_Jpsi_reco'),\n",
              "   Variable([-1, -0.46394, -0.12545, 0.13095, 0.33459, 0.50304, 0.64358, 0.76183, 0.861, 0.94157, 1], name='costheta_W_reco', label='costheta_W_reco'),\n",
              "   Variable([-3.14159, -2.42962, -1.84089, -1.30021, -0.71122, 0.0043844, 0.71329, 1.29936, 1.83985, 2.42724, 3.14159], name='chi_reco', label='chi_reco'),\n",
              "   storage=Weight()) # Sum: WeightedSum(value=0.00542589, variance=1.69023e-06) (WeightedSum(value=0.00554438, variance=1.69403e-06) with flow),\n",
              " 4: Hist(\n",
              "   Variable([-1, -0.79216, -0.58822, -0.38891, -0.19395, 0.00055617, 0.19486, 0.39035, 0.5888, 0.79242, 1], name='costheta_Jpsi_reco', label='costheta_Jpsi_reco'),\n",
              "   Variable([-1, -0.46394, -0.12545, 0.13095, 0.33459, 0.50304, 0.64358, 0.76183, 0.861, 0.94157, 1], name='costheta_W_reco', label='costheta_W_reco'),\n",
              "   Variable([-3.14159, -2.42962, -1.84089, -1.30021, -0.71122, 0.0043844, 0.71329, 1.29936, 1.83985, 2.42724, 3.14159], name='chi_reco', label='chi_reco'),\n",
              "   storage=Weight()) # Sum: WeightedSum(value=0.00779255, variance=1.12757e-06) (WeightedSum(value=0.0078214, variance=1.1284e-06) with flow),\n",
              " 5: Hist(\n",
              "   Variable([-1, -0.79216, -0.58822, -0.38891, -0.19395, 0.00055617, 0.19486, 0.39035, 0.5888, 0.79242, 1], name='costheta_Jpsi_reco', label='costheta_Jpsi_reco'),\n",
              "   Variable([-1, -0.46394, -0.12545, 0.13095, 0.33459, 0.50304, 0.64358, 0.76183, 0.861, 0.94157, 1], name='costheta_W_reco', label='costheta_W_reco'),\n",
              "   Variable([-3.14159, -2.42962, -1.84089, -1.30021, -0.71122, 0.0043844, 0.71329, 1.29936, 1.83985, 2.42724, 3.14159], name='chi_reco', label='chi_reco'),\n",
              "   storage=Weight()) # Sum: WeightedSum(value=-0.00847952, variance=7.19112e-07) (WeightedSum(value=-0.00841377, variance=7.19988e-07) with flow)}"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ],
      "source": [
        "#Load our histogram templates from previous notebook\n",
        "all_h_norm = {}\n",
        "hist_path = \"/content/drive/MyDrive/Bc2JpsiMuNu_Analysis/pickle\"\n",
        "for i in range(0,6):\n",
        "  with open(f\"{hist_path}/hist_{i}.pkl\", \"rb\") as f:\n",
        "    all_h_norm[i] = pickle.load(f)\n",
        "all_h_norm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load our MC, which we will use to create a dataset to fit. We randomly sample 100,000 events from our total MC sample, to act like a pretend LHCb dataset. We will want to fit the reconstructed decay angle distribution in this sample (like we would with a real dataset), to measure the helicity amplitdues."
      ],
      "metadata": {
        "id": "XXm-qY_oMm4s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UgRE659WU719",
        "outputId": "111906b9-845d-47b4-fce1-48eac57fa8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ROOT file /content/drive/MyDrive/Bc2JpsiMuNu_ROOT_files/Bc2JpsiMuNu_RapidSim_LHCb_Vars_Weights.root\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TTree 'DecayTree' (172 branches) at 0x7f42a736f310>"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ],
      "source": [
        "#Load our ROOT file containing the MC we want to fit\n",
        "drive_dir = \"/content/drive/MyDrive/Bc2JpsiMuNu_ROOT_files\"\n",
        "file_path = f\"{drive_dir}/Bc2JpsiMuNu_RapidSim_LHCb_Vars_Weights\"\n",
        "print(f\"Loading ROOT file {file_path}.root\")\n",
        "tree_name = \"DecayTree\"\n",
        "events = uproot.open(f\"{file_path}.root:{tree_name}\")\n",
        "events"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "V8sDq4CFWMZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "007edca0-7657-4b95-9ab1-743811dba834"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/uproot/interpretation/library.py:806: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  out[name] = series[name]\n"
          ]
        }
      ],
      "source": [
        "#Make pandas DataFrame\n",
        "df = events.arrays(library=\"pd\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPD_mu6iWODp",
        "outputId": "72a4fdc4-9ac9-4088-f51d-be8feceb460d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100000"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ],
      "source": [
        "#Downsample to DataFrame to 100k events, which will act as our fit dataset\n",
        "df_fit = df.sample(n=100000, random_state=42)\n",
        "len(df_fit)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we make a dictionary defining our x, y, and z fit variables, with some useful information:\n",
        "* `name`: the name of the variable in our ROOT file. Note the `reco` here for reconstructed.\n",
        "* `min` and `max`: the variable ranges.\n",
        "* `bins`: the number of bins for the variable.\n",
        "* `latex`: LaTeX name for use in plotting."
      ],
      "metadata": {
        "id": "I5qUvLBKMqa9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "PsKnkUwQWmX2"
      },
      "outputs": [],
      "source": [
        "#Define fit variables\n",
        "vars = {}\n",
        "vars[\"x_var\"] = {\"name\": \"costheta_Jpsi_reco\", \"min\": -1., \"max\": 1., \"bins\": 10, \"latex\": \"$\\\\cos(\\\\theta_{J/\\\\psi})$\"}\n",
        "vars[\"y_var\"] = {\"name\": \"costheta_W_reco\", \"min\": -1., \"max\": 1., \"bins\": 10, \"latex\": \"$\\\\cos(\\\\theta_{W})$\"}\n",
        "vars[\"z_var\"] = {\"name\": \"chi_reco\", \"min\": -np.pi, \"max\": np.pi, \"bins\": 10, \"latex\": \"$\\\\chi$ [rad]\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous notebook, we defined our template histograms with a binning scheme that puts roughly equal numbers of events into each bin. Let's reload the binnings that we saved, and use them again here for consistency."
      ],
      "metadata": {
        "id": "4UQQWplKMum3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXqp1fSCXFUK",
        "outputId": "a4f2c2bf-9524-4b2a-c530-294828ea36ab"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x_var': [-1.0,\n",
              "  -0.79216,\n",
              "  -0.58822,\n",
              "  -0.38891,\n",
              "  -0.19395,\n",
              "  0.00055617,\n",
              "  0.19486,\n",
              "  0.39035,\n",
              "  0.5888,\n",
              "  0.79242,\n",
              "  1.0],\n",
              " 'y_var': [-1.0,\n",
              "  -0.46394,\n",
              "  -0.12545,\n",
              "  0.13095,\n",
              "  0.33459,\n",
              "  0.50304,\n",
              "  0.64358,\n",
              "  0.76183,\n",
              "  0.861,\n",
              "  0.94157,\n",
              "  1.0],\n",
              " 'z_var': [-3.141592653589793,\n",
              "  -2.42962,\n",
              "  -1.84089,\n",
              "  -1.30021,\n",
              "  -0.71122,\n",
              "  0.0043844,\n",
              "  0.71329,\n",
              "  1.29936,\n",
              "  1.83985,\n",
              "  2.42724,\n",
              "  3.141592653589793]}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ],
      "source": [
        "#Get the binning schemes we used to make our templates (we saved them into a JSON file)\n",
        "json_path = \"/content/drive/MyDrive/Bc2JpsiMuNu_Analysis/json\"\n",
        "with open(f\"{json_path}/binnings.json\") as json_file:\n",
        "  binnings = json.load(json_file)\n",
        "binnings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the binned fit, we need to make a binned histogram of our fit dataset. This histogram should be defined using the same binning scheme we used to make our templates, which we have stored in the `binnings` dictionary.\n",
        "\n",
        "In `zfit`, like for our unbinned fit to the true angles, we define an observables space. We can then make an unbinned dataset first, and from that a binned one."
      ],
      "metadata": {
        "id": "Caet3xhGMxv6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec5b-1M1YaBa",
        "outputId": "edfeb06a-6adc-46d8-c1e8-5ce81447831f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/script_ops.py:626: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  result = func(*[np.array(x) for x in inp])\n"
          ]
        }
      ],
      "source": [
        "#Create a zfit binned dataset of the data, using the same binning as our templates (above)\n",
        "binning_x = zfit.binned.VariableBinning(binnings[\"x_var\"], name=vars[\"x_var\"][\"name\"])\n",
        "obs_x = zfit.Space(vars[\"x_var\"][\"name\"], binning=binning_x)\n",
        "\n",
        "binning_y = zfit.binned.VariableBinning(binnings[\"y_var\"], name=vars[\"y_var\"][\"name\"])\n",
        "obs_y = zfit.Space(vars[\"y_var\"][\"name\"], binning=binning_y)\n",
        "\n",
        "binning_z = zfit.binned.VariableBinning(binnings[\"z_var\"], name=vars[\"z_var\"][\"name\"])\n",
        "obs_z = zfit.Space(vars[\"z_var\"][\"name\"], binning=binning_z)\n",
        "\n",
        "obs = obs_x * obs_y * obs_z\n",
        "\n",
        "df_fit = df_fit[[\"costheta_Jpsi_reco\",\"costheta_W_reco\",\"chi_reco\"]]\n",
        "\n",
        "unbinned_data = zfit.Data.from_pandas(df_fit, obs=obs)\n",
        "\n",
        "binned_data = unbinned_data.to_binned(obs)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we convert out `Hist` template objects into `BinnedData` objects, which can be used by `zfit` inside our PDF to do calculations. "
      ],
      "metadata": {
        "id": "G6JuYyE2NH-v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAm8yfZqYuo7",
        "outputId": "6f7b4a6a-4ca9-476a-fa94-328e1e2f1979"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: <zfit._data.binneddatav1.BinnedData at 0x7f422b060a90>,\n",
              " 1: <zfit._data.binneddatav1.BinnedData at 0x7f422ad114d0>,\n",
              " 2: <zfit._data.binneddatav1.BinnedData at 0x7f422ad116d0>,\n",
              " 3: <zfit._data.binneddatav1.BinnedData at 0x7f422ad0c1d0>,\n",
              " 4: <zfit._data.binneddatav1.BinnedData at 0x7f422ad0cf50>,\n",
              " 5: <zfit._data.binneddatav1.BinnedData at 0x7f422ad0c0d0>}"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ],
      "source": [
        "#Create zfit binned data objects from each of our templates, which zfit can use as PDFs inside our total PDF definition\n",
        "hist_pdfs = {}\n",
        "for h in all_h_norm:\n",
        "  hist_pdfs[h] = zfit.data.BinnedData.from_hist(all_h_norm[h])\n",
        "hist_pdfs"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define our `zfit` fit parameters, which are the quantities we want to measure in the fit. We have the same parameters as in the unbinned fit, namely the helicity amplitude magnitudes and phases."
      ],
      "metadata": {
        "id": "JGlJgEijPN63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "WDZnsUrrbO9m"
      },
      "outputs": [],
      "source": [
        "#Helicity amplitude parameters\n",
        "#Random number to use in the param names, so we can run the fit lots of times\n",
        "rand = random.randint(0,100000)\n",
        "H0_amp = zfit.Parameter(f\"H0_amp_{rand}\", 0.7, 0., 1.)\n",
        "Hm_amp = zfit.Parameter(f\"Hm_amp_{rand}\", 0.6, 0., 1.)\n",
        "#One helicity amplitude is fixed by the fact that their squares must sum to 1\n",
        "def Hp_amp_func(H0_amp, Hm_amp):\n",
        "  return tf.sqrt(1. - H0_amp**2 - Hm_amp**2)\n",
        "Hp_amp = zfit.ComposedParameter(f\"Hp_amp_{rand}\", Hp_amp_func, params=[H0_amp, Hm_amp])\n",
        "\n",
        "#Phases - H0 phase is fixed to zero by convention\n",
        "H0_phi =  zfit.Parameter(f\"H0_phi_{rand}\", 0., floating=False)\n",
        "Hp_phi =  zfit.Parameter(f\"Hp_phi_{rand}\", 1.5, -2*np.pi, 2*np.pi)\n",
        "Hm_phi =  zfit.Parameter(f\"Hm_phi_{rand}\", -1.5,-2*np.pi, 2*np.pi)\n",
        "\n",
        "#Dictionary of all our parameters, which will be passed into our PDF builder function\n",
        "fit_params = {\"H0_amp\": H0_amp,\n",
        "              \"Hm_amp\": Hm_amp,\n",
        "              \"Hp_amp\": Hp_amp,\n",
        "              \"H0_phi\": H0_phi,\n",
        "              \"Hp_phi\": Hp_phi,\n",
        "              \"Hm_phi\": Hm_phi\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we define a class which will build our angular fit PDF for us. The total PDF will be given by a sum of our six templates, each multiplied by their corresponding piece of helicity amplitude.\n",
        "\n",
        "The main part to focus on is the function `_rel_counts`, where we define our PDF in a similar way to what we had in the unbinned fit. The main difference is the use of `templates[0]` e.t.c. to represent the various 3D angular functions. These templates are given to the PDF via the `hist_pdfs` dictionary we defined above, which itself contains six `BinnedData` objects (one for each template)."
      ],
      "metadata": {
        "id": "N6xXTZpbPe63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomPDF(zfit.core.binnedpdf.BaseBinnedPDFV1):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            templates,\n",
        "            params,\n",
        "            name: str = \"CustomPDF\"\n",
        "    ) -> None:\n",
        "        \"\"\"Total binned PDF of angular decay rate.\n",
        "        Args:\n",
        "            templates: Dictionary of histogram templates.\n",
        "            params: Dictionary of fit parameters.\n",
        "            name: |@doc:model.init.name| Human-readable name\n",
        "               or label of\n",
        "               the PDF for better identification.\n",
        "               Has no programmatical functional purpose as identification. |@docend:model.init.name|\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__(obs=obs, extended=None, norm=None, params=params, name=name)\n",
        "        self._templates = templates\n",
        "        self._params = params\n",
        "        \n",
        "\n",
        "    @supports(norm=False)\n",
        "    def _rel_counts(self, x, norm):\n",
        "      \n",
        "      #Complex numbers defined\n",
        "      h_0 = tf.complex(self._params[\"H0_amp\"]*tf.cos(self._params[\"H0_phi\"]),self._params[\"H0_amp\"]*tf.sin(self._params[\"H0_phi\"]))\n",
        "      h_p = tf.complex(self._params[\"Hp_amp\"]*tf.cos(self._params[\"Hp_phi\"]),self._params[\"Hp_amp\"]*tf.sin(self._params[\"Hp_phi\"]))\n",
        "      h_m = tf.complex(self._params[\"Hm_amp\"]*tf.cos(self._params[\"Hm_phi\"]),self._params[\"Hm_amp\"]*tf.sin(self._params[\"Hm_phi\"]))\n",
        "      \n",
        "      h_0st = tf.math.conj(h_0)\n",
        "      h_pst = tf.math.conj(h_p)\n",
        "      h_mst = tf.math.conj(h_m)\n",
        "      \n",
        "      HpHmst = h_p*h_mst\n",
        "      HpH0st = h_p*h_0st\n",
        "      HmH0st = h_m*h_0st\n",
        "      \n",
        "      #Total PDF given by a sum over each template multiplied by its corresponding bit of helicity amplitude\n",
        "      #.counts() returns the bin contents which zfit can work with to build the summed PDF\n",
        "      pdf = self._params[\"H0_amp\"]**2 * 2 * self._templates[0].counts()\n",
        "      pdf += self._params[\"Hp_amp\"]**2 * 0.5 * self._templates[1].counts()\n",
        "      pdf += self._params[\"Hm_amp\"]**2 * 0.5 * self._templates[2].counts()\n",
        "      pdf += tf.math.real(HpH0st) * self._templates[3].counts()\n",
        "      pdf += -tf.math.real(HmH0st) * self._templates[4].counts()\n",
        "      pdf += tf.math.real(HpHmst) * self._templates[5].counts()\n",
        "\n",
        "      return pdf"
      ],
      "metadata": {
        "id": "pvdNUAHaRvpj"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we make an instance of our custom PDF, which we can use in the fit to measure the helicity ampltidue parameters."
      ],
      "metadata": {
        "id": "MbcwJzIdQF7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create an instance of our PDF class for use in the fit\n",
        "tot_pdf = CustomPDF(templates=hist_pdfs, params=fit_params)"
      ],
      "metadata": {
        "id": "a9gw5kn9YgTt"
      },
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbUA_W398jWf",
        "outputId": "f3b5aa46-fc4d-46e5-9e7c-62f274d3f63e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Function minimum: -358284.8253828372\n",
            "Converged: True\n",
            "Full minimizer information: {'n_eval': 18, 'minuit': <FMin algorithm='Migrad' edm=3.117240434587433e-07 edm_goal=0.001 errordef=0.5 fval=-358284.8253828372 has_accurate_covar=True has_covariance=True has_made_posdef_covar=False has_parameters_at_limit=False has_posdef_covar=True has_reached_call_limit=False has_valid_parameters=True hesse_failed=False is_above_max_edm=False is_valid=True nfcn=41 ngrad=1 reduced_chi2=nan>\n",
            "(Param(number=0, name='H0_amp_12982', value=0.6842977264274288, error=0.003636340544758754, merror=None, is_const=False, is_fixed=False, lower_limit=0.0, upper_limit=1.0), Param(number=1, name='Hm_amp_12982', value=0.6426092892023508, error=0.0028818896377998393, merror=None, is_const=False, is_fixed=False, lower_limit=0.0, upper_limit=1.0), Param(number=2, name='Hp_phi_12982', value=1.5384343552258672, error=0.05305173480188807, merror=None, is_const=False, is_fixed=False, lower_limit=-6.2831854820251465, upper_limit=6.2831854820251465), Param(number=3, name='Hm_phi_12982', value=-1.518113114258089, error=0.03186759586850241, merror=None, is_const=False, is_fixed=False, lower_limit=-6.2831854820251465, upper_limit=6.2831854820251465))\n",
            "[[ 1.32232423e-05 -8.98699968e-06  5.00369267e-06 -2.96778172e-06]\n",
            " [-8.98699968e-06  8.30538800e-06 -3.14661494e-06  1.59860689e-06]\n",
            " [ 5.00369267e-06 -3.14661494e-06  2.81455772e-03 -1.17648700e-03]\n",
            " [-2.96778172e-06  1.59860689e-06 -1.17648700e-03  1.01555291e-03]], 'original': <FMin algorithm='Migrad' edm=3.117240434587433e-07 edm_goal=0.001 errordef=0.5 fval=-358284.8253828372 has_accurate_covar=True has_covariance=True has_made_posdef_covar=False has_parameters_at_limit=False has_posdef_covar=True has_reached_call_limit=False has_valid_parameters=True hesse_failed=False is_above_max_edm=False is_valid=True nfcn=18 ngrad=1 reduced_chi2=nan>, 'inv_hessian': array([[ 1.77885443e-06, -1.09476521e-06,  5.42628324e-06,\n",
            "         8.94006950e-07],\n",
            "       [-1.09476521e-06,  1.50280448e-06,  3.30900580e-06,\n",
            "         7.83083311e-07],\n",
            "       [ 5.42628324e-06,  3.30900580e-06,  1.44152349e-03,\n",
            "        -1.32416748e-05],\n",
            "       [ 8.94006950e-07,  7.83083311e-07, -1.32416748e-05,\n",
            "         5.38208436e-04]])}\n",
            "name            value    minuit_hesse    at limit\n",
            "------------  -------  --------------  ----------\n",
            "H0_amp_12982   0.6843     +/-  0.0036       False\n",
            "Hm_amp_12982   0.6426     +/-  0.0029       False\n",
            "Hp_phi_12982    1.538     +/-   0.053       False\n",
            "Hm_phi_12982   -1.518     +/-   0.032       False\n"
          ]
        }
      ],
      "source": [
        "#Run the fit \n",
        "\n",
        "# Stage 1: create a binned likelihood with the given PDF and dataset\n",
        "nll = zfit.loss.BinnedNLL(tot_pdf, binned_data)\n",
        "\n",
        "# Stage 2: instantiate a minimiser (in this case a basic minuit)\n",
        "minimizer = zfit.minimize.Minuit(gradient=False)\n",
        "\n",
        "#Stage 3: minimise the given negative likelihood\n",
        "result = minimizer.minimize(nll)\n",
        "\n",
        "#Get the parameter uncertainties using Hesse\n",
        "param_errors = result.hesse(method=\"minuit_hesse\")\n",
        "\n",
        "print(\"Function minimum:\", result.fmin)\n",
        "print(\"Converged:\", result.converged)\n",
        "print(\"Full minimizer information:\", result.info)\n",
        "\n",
        "result_params = result.params\n",
        "print(result_params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Put results into a dictionary\n",
        "results_dict = {}\n",
        "for p in result_params:\n",
        "    par_name = p.name\n",
        "    #Remove the random piece of the name which we added to allow zfit to run many times\n",
        "    par_name = par_name.replace(\"_\"+str(rand),\"\")\n",
        "    results_dict[par_name] = [result_params[p]['value'], param_errors[p][\"error\"]]\n",
        "results_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKgPJJSlZGrM",
        "outputId": "4aa6dada-3532-4259-bb3f-5c8ff3cb8a3e"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'H0_amp': [0.6842977264274288, 0.003636377625082117],\n",
              " 'Hm_amp': [0.6426092892023508, 0.0028819069949844135],\n",
              " 'Hm_phi': [-1.518113114258089, 0.03186774082716549],\n",
              " 'Hp_phi': [1.5384343552258672, 0.053052405150084525]}"
            ]
          },
          "metadata": {},
          "execution_count": 156
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the parameter H+ based on the values of H0 and H-, using the Python uncertainties package to propagate uncertainties for us\n",
        "#Here, results_dict[\"H0_amp\"][0] gets us the value of H0_amp from the fit\n",
        "#results_dict[\"H0_amp\"][1] gets us the error\n",
        "#ufloat is an uncertainties object, which has a central value (the first value) and an uncertainty (the second value)\n",
        "v_H0_amp = ufloat(results_dict[\"H0_amp\"][0], results_dict[\"H0_amp\"][1])\n",
        "v_Hm_amp = ufloat(results_dict[\"Hm_amp\"][0], results_dict[\"Hm_amp\"][1])\n",
        "\n",
        "v_H0_amp, v_Hm_amp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu3tbRDKfcxt",
        "outputId": "0bfade2a-2f36-4e71-f9a4-cc9fcf563eac"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6842977264274288+/-0.003636377625082117,\n",
              " 0.6426092892023508+/-0.0028819069949844135)"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate a new ufloat for Hp_amp, using the formula Hp_amp = sqrt(1 - H0_amp^2 - Hm_amp^2) [since all three squares of the H_amp sum to 1]\n",
        "v_Hp_amp = (1. - v_H0_amp**2 - v_Hm_amp**2)**(1./2.)\n",
        "v_Hp_amp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ul68qXnfkWc",
        "outputId": "c98994e3-3345-44b9-9ec1-bcf7a830cc54"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.34465914036494266+/-0.00899984044523662"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add the value of Hp_amp into our dictionary (n gives us its nominal value, and s its standard deviation)\n",
        "results_dict[\"Hp_amp\"] = [v_Hp_amp.n, v_Hp_amp.s]\n",
        "results_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2py3pbOfq1L",
        "outputId": "34a36e9f-75a7-41eb-bb5b-ee4217133211"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'H0_amp': [0.6842977264274288, 0.003636377625082117],\n",
              " 'Hm_amp': [0.6426092892023508, 0.0028819069949844135],\n",
              " 'Hm_phi': [-1.518113114258089, 0.03186774082716549],\n",
              " 'Hp_amp': [0.34465914036494266, 0.00899984044523662],\n",
              " 'Hp_phi': [1.5384343552258672, 0.053052405150084525]}"
            ]
          },
          "metadata": {},
          "execution_count": 159
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write fit results dictionary to a JSON file, which we can use later in other analyses\n",
        "ana_dir = \"/content/drive/MyDrive/Bc2JpsiMuNu_Analysis\"\n",
        "file_path = f\"{ana_dir}/json/Bc2JspiMuNu_RapidSim_binned_fit_new_results.json\"\n",
        "with open(file_path, 'w') as f:\n",
        "  json.dump(results_dict, f, sort_keys=True, indent=4)"
      ],
      "metadata": {
        "id": "4rAFknXBftaz"
      },
      "execution_count": 160,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Bc2JpsiMuNu_RapidSim_LHCb_binned_fit_new.ipynb",
      "provenance": [],
      "mount_file_id": "1vf5CvxgaNy5XeAOYAaBC6nGaBVVkKk2L",
      "authorship_tag": "ABX9TyOpu3Fg7sDOruLDhulXZvUq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}