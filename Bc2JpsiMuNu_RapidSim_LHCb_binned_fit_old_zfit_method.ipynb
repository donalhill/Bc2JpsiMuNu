{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bc2JpsiMuNu_RapidSim_LHCb_binned_fit_old_zfit_method.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NCeP5xteyzak2c6-6qzlKEfL_c5QWyow",
      "authorship_tag": "ABX9TyN00ckFhyoAkhvKP3mKHPpu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/donalrinho/Bc2JpsiMuNu/blob/main/Bc2JpsiMuNu_RapidSim_LHCb_binned_fit_old_zfit_method.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTo7M6ll1rCM",
        "outputId": "fb90055c-0048-4d03-9e5e-4e22545cf94c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 301 kB 7.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 458.3 MB 12 kB/s \n",
            "\u001b[K     |████████████████████████████████| 132 kB 36.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.6 MB 13.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 71.9 MB/s \n",
            "\u001b[?25h  Building wheel for clang (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 15.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 12.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 31.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 895 kB 58.2 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 332 kB 8.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 420 kB 33.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 339 kB 61.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 45.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 54.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 48.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 21.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 189 kB 46.5 MB/s \n",
            "\u001b[?25h  Building wheel for colored (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for algopy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 98 kB 3.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q uproot\n",
        "!pip install -q tensorflow==2.6.2 #specific versions for compatability with zfit\n",
        "!pip install -q hist\n",
        "!pip install -q mplhep\n",
        "!pip install -q zfit #git+https://github.com/zfit/zfit #development version of zfit needed to get binned fit tools\n",
        "!pip install -q uncertainties"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import uproot\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import zfit\n",
        "import hist\n",
        "from hist import Hist\n",
        "import mplhep\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import json\n",
        "import random\n",
        "from uncertainties import *"
      ],
      "metadata": {
        "id": "G5hn9PZt1u5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0802979f-81a0-42ec-a6d0-d70f78c6756f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/zfit/__init__.py:37: UserWarning: TensorFlow warnings are by default suppressed by zfit. In order to show them, set the environment variable ZFIT_DISABLE_TF_WARNINGS=0. In order to suppress the TensorFlow warnings AND this warning, set ZFIT_DISABLE_TF_WARNINGS=1.\n",
            "  warnings.warn(\"TensorFlow warnings are by default suppressed by zfit.\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load our histogram templates from previous notebook\n",
        "all_h_norm = {}\n",
        "hist_path = \"/content/drive/MyDrive/Bc2JpsiMuNu_Analysis/pickle\"\n",
        "for i in range(0,6):\n",
        "  with open(f\"{hist_path}/hist_{i}.pkl\", \"rb\") as f:\n",
        "    all_h_norm[i] = pickle.load(f)\n",
        "    all_h_norm[i] = all_h_norm[i].values() #convert to numpy arrays"
      ],
      "metadata": {
        "id": "9K25YmVn238T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load our ROOT file containing the MC we want to fit\n",
        "drive_dir = \"/content/drive/MyDrive/Bc2JpsiMuNu_ROOT_files\"\n",
        "file_path = f\"{drive_dir}/Bc2JpsiMuNu_RapidSim_LHCb_Vars_Weights\"\n",
        "print(f\"Loading ROOT file {file_path}.root\")\n",
        "tree_name = \"DecayTree\"\n",
        "events = uproot.open(f\"{file_path}.root:{tree_name}\")\n",
        "events\n",
        "\n",
        "#Make pandas DataFrame\n",
        "df = events.arrays(library=\"pd\")\n",
        "\n",
        "#Downsample to DataFrame to 100k events, which will act as our fit dataset\n",
        "df_fit = df.sample(n=100000, random_state=42)\n",
        "n_data = len(df_fit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ahT8Njd26_n",
        "outputId": "f2ac1262-eaf6-445f-96cb-73f96eed257d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading ROOT file /content/drive/MyDrive/Bc2JpsiMuNu_ROOT_files/Bc2JpsiMuNu_RapidSim_LHCb_Vars_Weights.root\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/uproot/interpretation/library.py:806: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  out[name] = series[name]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define fit variables dict containing useful properties like name, min, max, LaTeX title\n",
        "vars = {}\n",
        "vars[\"x_var\"] = {\"name\": \"costheta_Jpsi_reco\", \"min\": -1., \"max\": 1., \"bins\": 10, \"latex\": \"$\\\\cos(\\\\theta_{J/\\\\psi})$\"}\n",
        "vars[\"y_var\"] = {\"name\": \"costheta_W_reco\", \"min\": -1., \"max\": 1., \"bins\": 10, \"latex\": \"$\\\\cos(\\\\theta_{W})$\"}\n",
        "vars[\"z_var\"] = {\"name\": \"chi_reco\", \"min\": -np.pi, \"max\": np.pi, \"bins\": 10, \"latex\": \"$\\\\chi$ [rad]\"}"
      ],
      "metadata": {
        "id": "OPloOEGa3GNa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the binning schemes we used to make our templates (we saved them into a JSON file)\n",
        "json_path = \"/content/drive/MyDrive/Bc2JpsiMuNu_Analysis/json\"\n",
        "with open(f\"{json_path}/binnings.json\") as json_file:\n",
        "  binnings = json.load(json_file)\n",
        "binnings"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1A9NAN53PYp",
        "outputId": "f4ffbf75-13cd-4803-8d1e-c5143dbb2739"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x_var': [-1.0,\n",
              "  -0.79216,\n",
              "  -0.58822,\n",
              "  -0.38891,\n",
              "  -0.19395,\n",
              "  0.00055617,\n",
              "  0.19486,\n",
              "  0.39035,\n",
              "  0.5888,\n",
              "  0.79242,\n",
              "  1.0],\n",
              " 'y_var': [-1.0,\n",
              "  -0.46394,\n",
              "  -0.12545,\n",
              "  0.13095,\n",
              "  0.33459,\n",
              "  0.50304,\n",
              "  0.64358,\n",
              "  0.76183,\n",
              "  0.861,\n",
              "  0.94157,\n",
              "  1.0],\n",
              " 'z_var': [-3.141592653589793,\n",
              "  -2.42962,\n",
              "  -1.84089,\n",
              "  -1.30021,\n",
              "  -0.71122,\n",
              "  0.0043844,\n",
              "  0.71329,\n",
              "  1.29936,\n",
              "  1.83985,\n",
              "  2.42724,\n",
              "  3.141592653589793]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Make a hist of the dataset we will fit, filling it with our MC\n",
        "data_h = (\n",
        "    Hist.new\n",
        "    .Variable(binnings[\"x_var\"], name=vars[\"x_var\"][\"name\"])\n",
        "    .Variable(binnings[\"y_var\"], name=vars[\"y_var\"][\"name\"])\n",
        "    .Variable(binnings[\"z_var\"], name=vars[\"z_var\"][\"name\"])\n",
        "    .Weight()\n",
        "    )\n",
        "data_h.fill(df_fit[vars[\"x_var\"][\"name\"]], \n",
        "            df_fit[vars[\"y_var\"][\"name\"]], \n",
        "            df_fit[vars[\"z_var\"][\"name\"]])\n",
        "data_h = data_h.values() #convert to numpy array"
      ],
      "metadata": {
        "id": "-QWULwmZ3Q7l"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Binned maximum likelihood function which we will minimise (in order to maximise the likelihood)\n",
        "def binned_nll(pdf, data):\n",
        "      return np.sum(pdf - data + data * np.log((data + 1e-14) / (pdf + 1e-14)))\n",
        "      # 1e-14 added in case there are empty bins"
      ],
      "metadata": {
        "id": "pXMJoyq64Ao6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define our fit PDF as a sum of histogram templates, the binned analogue of what we did in our unbinned fit to the true angles\n",
        "def make_pdf(params, templates):\n",
        "  \n",
        "  #Fit parameters, the angular coefficients\n",
        "  H0_amp = params['H0_amp']\n",
        "  Hm_amp = params['Hm_amp']\n",
        "\n",
        "  Hp_phi = params['Hp_phi']\n",
        "  Hm_phi = params['Hm_phi']\n",
        "\n",
        "  #Fixed and derived parameters\n",
        "  H0_phi = 0.\n",
        "  Hp_amp = float(np.sqrt(1. - H0_amp**2 - Hm_amp**2))\n",
        "\n",
        "  h_0 = tf.complex(H0_amp*np.cos(H0_phi),H0_amp*np.sin(H0_phi))\n",
        "  h_p = tf.complex(Hp_amp*np.cos(Hp_phi),Hp_amp*np.sin(Hp_phi))\n",
        "  h_m = tf.complex(Hm_amp*np.cos(Hm_phi),Hm_amp*np.sin(Hm_phi))\n",
        "  \n",
        "  h_0st = tf.math.conj(h_0)\n",
        "  h_pst = tf.math.conj(h_p)\n",
        "  h_mst = tf.math.conj(h_m)\n",
        "  \n",
        "  HpHmst = h_p*h_mst\n",
        "  HpH0st = h_p*h_0st\n",
        "  HmH0st = h_m*h_0st\n",
        "\n",
        "  pdf = H0_amp**2 * 2 * templates[0]\n",
        "  pdf += Hp_amp**2 * 0.5 * templates[1]\n",
        "  pdf += Hm_amp**2 * 0.5 * templates[2]\n",
        "  pdf += float(tf.math.real(HpH0st)) * templates[3]\n",
        "  pdf += -float(tf.math.real(HmH0st)) * templates[4]\n",
        "  pdf += float(tf.math.real(HpHmst)) * templates[5]\n",
        "\n",
        "  #Normalise the PDF\n",
        "  pdf = pdf / np.sum(pdf)\n",
        "  \n",
        "  return pdf"
      ],
      "metadata": {
        "id": "QiGsRIPE7tOD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define loss function\n",
        "def loss(x):\n",
        "  # by default, x is an OrderedSet of zfit parameters\n",
        "  # The order of x matches the order of the params dict\n",
        "  x = np.array(x)\n",
        "\n",
        "  #print(\"Value of the parameters\", x)\n",
        "\n",
        "  #Put the fit parameters from x into a dict of variables, which is passed to the PDF maker function, make_pdf()\n",
        "  pars_dict = {}\n",
        "\n",
        "  pars_dict[\"H0_amp\"] = x[0]\n",
        "  pars_dict[\"Hm_amp\"] = x[1]\n",
        "  pars_dict[\"Hp_phi\"] = x[2]\n",
        "  pars_dict[\"Hm_phi\"] = x[3]\n",
        " \n",
        "  pdf = make_pdf(pars_dict, all_h_norm) #Pass our dict of params and our dict of histogram templates\n",
        "\n",
        "  #Scale PDF to match the dataset size\n",
        "  pdf = pdf * n_data\n",
        "\n",
        "  #Binned log-likelihood, passing the total PDF and the dataset\n",
        "  nll = binned_nll(pdf, data_h)\n",
        "\n",
        "  return nll"
      ],
      "metadata": {
        "id": "0H6Vk4ey_ykm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial parameter values\n",
        "init_vals = {\"H0_amp\": 0.7,\n",
        "             \"Hm_amp\": 0.6,\n",
        "             \"Hp_phi\": 1.5,\n",
        "             \"Hm_phi\": -1.5\n",
        "}"
      ],
      "metadata": {
        "id": "MfdKrWq__Jr_"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit parameters dictionary\n",
        "rand = random.randint(0,100000)\n",
        "params = {\"value\": [init_vals[\"H0_amp\"], init_vals[\"Hm_amp\"], init_vals[\"Hp_phi\"], init_vals[\"Hm_phi\"]],\n",
        "          \"lower\": [0., 0., -2*np.pi, -2*np.pi],\n",
        "          \"upper\": [1., 1., 2*np.pi, 2*np.pi],\n",
        "          \"name\": [f\"H0_amp_{rand}\", f\"Hm_amp_{rand}\", f\"Hp_phi_{rand}\", f\"Hm_phi_{rand}\"]\n",
        "}"
      ],
      "metadata": {
        "id": "jC52NpO59IDp"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test our building of the PDF and that it returns a sensible nll and chi2/dof value\n",
        "pdf_test = make_pdf(init_vals, all_h_norm)\n",
        "#Scale to match the yield of the dataset we are fitting\n",
        "pdf_test = pdf_test * n_data\n",
        "\n",
        "#Compute NLL of data and test PDF (initialised with values from the unbinned fit)\n",
        "nll = binned_nll(pdf_test, data_h)\n",
        "print(f\"NLL = {nll}\")\n",
        "\n",
        "#Chi2 sum over all bins\n",
        "chi2 = np.sum((pdf_test - data_h)**2 / data_h)\n",
        "#Reduced chi2\n",
        "chi2_dof = chi2 / data_h.size\n",
        "print(f\"chi2/dof = {chi2_dof}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DcrG8uCKfL2w",
        "outputId": "55d740d9-4df1-4d69-d582-8d02b4e64475"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLL = 637.2926914668059\n",
            "chi2/dof = 1.3073173925894792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Run fit\n",
        "loss.errordef = 0.5 # 0.5 for a log-likelihood, 1 for chi2\n",
        "\n",
        "minimiser = zfit.minimize.Minuit(verbosity=5)\n",
        "#Since we're using numpy histograms, we need to disable the graph mode of zfit\n",
        "zfit.run.set_autograd_mode(False)\n",
        "zfit.run.set_graph_mode(False)\n",
        "\n",
        "#Pass the dict of parameters with inital values, as defined in defs.py\n",
        "result = minimiser.minimize(loss, params)\n",
        "param_errors = result.hesse(method=\"minuit_hesse\")\n",
        "corr = result.correlation(method=\"minuit_hesse\")\n",
        "cov = result.covariance(method=\"minuit_hesse\")\n",
        "\n",
        "print(result.info['original'])\n",
        "\n",
        "result_params = result.params\n",
        "\n",
        "for p in result_params:\n",
        "  print(f\"{p.name}: {result_params[p.name]['value']} +/- {result_params[p.name]['minuit_hesse']['error']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkxEtvp4DZIr",
        "outputId": "15a11a51-c407-470e-bd2f-40235155e0d4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌─────────────────────────────────────────────────────────────────────────┐\n",
            "│                                Migrad                                   │\n",
            "├──────────────────────────────────┬──────────────────────────────────────┤\n",
            "│ FCN = 423.9                      │              Nfcn = 100              │\n",
            "│ EDM = 2.98e-05 (Goal: 0.001)     │                                      │\n",
            "├──────────────────────────────────┼──────────────────────────────────────┤\n",
            "│          Valid Minimum           │        No Parameters at limit        │\n",
            "├──────────────────────────────────┼──────────────────────────────────────┤\n",
            "│ Below EDM threshold (goal x 10)  │           Below call limit           │\n",
            "├───────────────┬──────────────────┼───────────┬─────────────┬────────────┤\n",
            "│  Covariance   │     Hesse ok     │ Accurate  │  Pos. def.  │ Not forced │\n",
            "└───────────────┴──────────────────┴───────────┴─────────────┴────────────┘\n",
            "H0_amp_69578: 0.6842905569840235 +/- 0.003584354066448079\n",
            "Hm_amp_69578: 0.6426035557173815 +/- 0.0028407413743337675\n",
            "Hp_phi_69578: 1.5384539453990644 +/- 0.05302641082634226\n",
            "Hm_phi_69578: -1.518089147445701 +/- 0.03185314372137788\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Put results into a dictionary\n",
        "results_dict = {}\n",
        "for p in result_params:\n",
        "    par_name = p.name\n",
        "    #Remove the random piece of the name which we added to allow zfit to run many times\n",
        "    par_name = par_name.replace(\"_\"+str(rand),\"\")\n",
        "    results_dict[par_name] = [result_params[p]['value'], param_errors[p][\"error\"]]\n",
        "results_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxGtdzNDp1tW",
        "outputId": "4f738c50-84d1-4dc4-d3c5-4d7a8233e21d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'H0_amp': [0.6842905569840235, 0.003584354066448079],\n",
              " 'Hm_amp': [0.6426035557173815, 0.0028407413743337675],\n",
              " 'Hm_phi': [-1.518089147445701, 0.03185314372137788],\n",
              " 'Hp_phi': [1.5384539453990644, 0.05302641082634226]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the parameter H+ based on the values of H0 and H-, using the Python uncertainties package to propagate uncertainties for us\n",
        "#Here, results_dict[\"H0_amp\"][0] gets us the value of H0_amp from the fit\n",
        "#results_dict[\"H0_amp\"][1] gets us the error\n",
        "#ufloat is an uncertainties object, which has a central value (the first value) and an uncertainty (the second value)\n",
        "v_H0_amp = ufloat(results_dict[\"H0_amp\"][0], results_dict[\"H0_amp\"][1])\n",
        "v_Hm_amp = ufloat(results_dict[\"Hm_amp\"][0], results_dict[\"Hm_amp\"][1])\n",
        "\n",
        "v_H0_amp, v_Hm_amp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59g-aFM3p4B0",
        "outputId": "d32ce7ed-496c-40e5-f624-144f7ea0e551"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.6842905569840235+/-0.003584354066448079,\n",
              " 0.6426035557173815+/-0.0028407413743337675)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate a new ufloat for Hp_amp, using the formula Hp_amp = sqrt(1 - H0_amp^2 - Hm_amp^2) [since all three squares of the H_amp sum to 1]\n",
        "v_Hp_amp = (1. - v_H0_amp**2 - v_Hm_amp**2)**(1./2.)\n",
        "v_Hp_amp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z03hqytJqZEX",
        "outputId": "87259ea4-b54c-4392-a3a1-e243894ebd1e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3446840637480547+/-0.008870426863269882"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Add the value of Hp_amp into our dictionary (n gives us its nominal value, and s its standard deviation)\n",
        "results_dict[\"Hp_amp\"] = [v_Hp_amp.n, v_Hp_amp.s]\n",
        "results_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h59lHdWsqgls",
        "outputId": "d99a3609-3901-451e-9a67-ea671822b205"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'H0_amp': [0.6842905569840235, 0.003584354066448079],\n",
              " 'Hm_amp': [0.6426035557173815, 0.0028407413743337675],\n",
              " 'Hm_phi': [-1.518089147445701, 0.03185314372137788],\n",
              " 'Hp_amp': [0.3446840637480547, 0.008870426863269882],\n",
              " 'Hp_phi': [1.5384539453990644, 0.05302641082634226]}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Write fit results dictionary to a JSON file, which we can use later in other analyses\n",
        "ana_dir = \"/content/drive/MyDrive/Bc2JpsiMuNu_Analysis\"\n",
        "file_path = f\"{ana_dir}/json/Bc2JspiMuNu_RapidSim_binned_fit_results.json\"\n",
        "with open(file_path, 'w') as f:\n",
        "  json.dump(results_dict, f, sort_keys=True, indent=4)"
      ],
      "metadata": {
        "id": "Avf4cuU4qjJ0"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}